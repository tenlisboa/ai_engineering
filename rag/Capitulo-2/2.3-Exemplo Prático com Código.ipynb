{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Pode falar sobre energia renovável, além de efeito estufa?\n",
      "Resposta: em energia renovável pode impulsionar a economia verde\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simulando uma base de dados de documentos\n",
    "documents = [\n",
    "    \"A energia renovável é obtida de recursos naturais que se regeneram naturalmente.\",\n",
    "    \"Os principais tipos de energia renovável são solar, eólica, hidroelétrica e biomassa.\",\n",
    "    \"A energia renovável ajuda a reduzir a emissão de gases de efeito estufa.\",\n",
    "    \"Investir em energia renovável pode impulsionar a economia verde.\"\n",
    "]\n",
    "\n",
    "# Passo 1: Indexação dos documentos usando embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "document_embeddings = embedder.encode(documents)\n",
    "\n",
    "# Criação do índice FAISS\n",
    "d = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Função para recuperar documentos relevantes\n",
    "def retrieve_documents(query, k=2):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)    \n",
    "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "# Carregando o modelo gerador (usando T5 para resumo)\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "\n",
    "# Função para gerar resposta\n",
    "def generate_answer(question, retrieved_docs):\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    input_text = f\"summarize: question: {question} context: {context}\"    \n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Pode falar sobre energia renovável, além de efeito estufa?\"\n",
    "    retrieved_docs = retrieve_documents(question)\n",
    "    answer = generate_answer(question, retrieved_docs)\n",
    "    print(\"Pergunta:\", question)\n",
    "    print(\"Resposta:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta arquitetura combina recuperação de informações com geração de texto para produzir respostas mais informadas e contextualizadas.\n",
    "\n",
    "1. Base de Dados:\n",
    "   - Simulada por uma lista de strings chamada `documents`.\n",
    "   - Cada string representa um documento com informações sobre energia renovável.\n",
    "\n",
    "2. Indexação e Recuperação:\n",
    "   - Utiliza o modelo `SentenceTransformer('all-MiniLM-L6-v2')` para criar embeddings dos documentos.\n",
    "   - Os embeddings são armazenados em um índice FAISS (Facebook AI Similarity Search) para busca eficiente.\n",
    "   - A função `retrieve_documents()` realiza a recuperação:\n",
    "     - Converte a query em um embedding.\n",
    "     - Usa o índice FAISS para encontrar os k documentos mais similares.\n",
    "\n",
    "3. Geração de Resposta:\n",
    "   - Utiliza o modelo T5 ('t5-small') para gerar respostas.\n",
    "   - A função `generate_answer()`:\n",
    "     - Combina a pergunta e os documentos recuperados em um único texto de entrada.\n",
    "     - Usa o tokenizer T5 para preparar os dados para o modelo.\n",
    "     - Gera uma resposta usando o modelo T5.\n",
    "\n",
    "4. Fluxo de Execução:\n",
    "   a. O usuário faz uma pergunta.\n",
    "   b. A pergunta é passada para `retrieve_documents()` para encontrar documentos relevantes.\n",
    "   c. Os documentos recuperados e a pergunta são passados para `generate_answer()`.\n",
    "   d. O modelo T5 gera uma resposta baseada na pergunta e nos documentos recuperados.\n",
    "\n",
    "5. Componentes Principais:\n",
    "   - Embedder: SentenceTransformer para criar representações vetoriais dos textos.\n",
    "   - Índice: FAISS para busca eficiente de similaridade.\n",
    "   - Gerador: Modelo T5 para geração de texto.\n",
    "   - Tokenizer: Prepara os dados para o modelo T5.\n",
    "\n",
    "6. Particularidades:\n",
    "   - Usa \"summarize:\" como prefixo na entrada do T5, orientando-o a gerar um resumo.\n",
    "   - Limita o tamanho máximo da entrada e da saída para evitar problemas com sequências muito longas.\n",
    "   - Utiliza beam search (num_beams=5) para melhorar a qualidade da geração.\n",
    "\n",
    "Esta arquitetura permite que o sistema combine informações de múltiplos documentos relevantes para gerar uma resposta coerente e informativa, superando as limitações de modelos de linguagem treinados apenas com conhecimento estático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
