{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \"routellm[serve,eval]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Script de Introdução ao Roteamento de LLMs com o RouteLLM**\n",
    "\n",
    "O avanço dos Modelos de Linguagem de Grande Porte (LLMs) tem revolucionado a forma como interagimos com sistemas de inteligência artificial. No entanto, modelos mais poderosos, como o GPT-4, são computacionalmente intensivos e caros, enquanto modelos menores podem não oferecer o mesmo nível de desempenho. Para equilibrar custo e qualidade, surge o conceito de **roteamento de LLMs**, onde diferentes modelos são utilizados com base na complexidade da consulta.\n",
    "\n",
    "O **RouteLLM** é um framework que implementa esse conceito, permitindo que as aplicações decidam automaticamente qual modelo utilizar para cada consulta, otimizando tanto a eficiência quanto a qualidade das respostas.\n",
    "\n",
    "---\n",
    "\n",
    "**Análise Conceitual do Script**\n",
    "\n",
    "Vamos analisar o script fornecido para entender como o roteamento de LLMs é implementado com o RouteLLM.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import yaml\n",
    "from routellm.controller import Controller\n",
    "\n",
    "# Define a chave da API da OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Define os modelos forte e fraco\n",
    "strong_model = \"gpt-4o\"      # Modelo forte (mais capaz, custo mais alto)\n",
    "weak_model = \"gpt-4o-mini\"   # Modelo fraco (menos capaz, custo mais baixo)\n",
    "```\n",
    "\n",
    "1. **Configuração Inicial**\n",
    "\n",
    "   - **Importações**: Carregamos as bibliotecas necessárias, incluindo `os` para manipular variáveis de ambiente, `yaml` para lidar com arquivos de configuração e `Controller` do RouteLLM para gerenciar o roteamento.\n",
    "   - **Chave de API**: Definimos a chave da API da OpenAI para autenticação nas chamadas aos modelos.\n",
    "\n",
    "2. **Definição dos Modelos**\n",
    "\n",
    "   - **Modelos**: Especificamos o modelo forte (`gpt-4o`) e o modelo fraco (`gpt-4o-mini`).\n",
    "     - **Modelo Forte**: Mais poderoso, capaz de lidar com consultas complexas, mas com custo operacional maior.\n",
    "     - **Modelo Fraco**: Menos potente, adequado para consultas simples, com menor custo.\n",
    "\n",
    "```python\n",
    "# Carrega a configuração do arquivo YAML\n",
    "try:\n",
    "    with open('config.example.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo de configuração 'config.example.yaml' não encontrado. Usando a configuração padrão.\")\n",
    "    config = None\n",
    "```\n",
    "\n",
    "3. **Carregamento da Configuração**\n",
    "\n",
    "   - **Configuração**: Tentamos carregar parâmetros adicionais de configuração a partir de um arquivo YAML.\n",
    "   - **Fallback**: Se o arquivo não for encontrado, usamos as configurações padrão embutidas no RouteLLM.\n",
    "\n",
    "```python\n",
    "# Inicializa o Controller com o roteador MF\n",
    "client = Controller(\n",
    "    routers=[\"mf\"],          # Usa o roteador de Fatoração de Matrizes\n",
    "    strong_model=strong_model,\n",
    "    weak_model=weak_model,\n",
    "    config=config\n",
    ")\n",
    "```\n",
    "\n",
    "4. **Inicialização do Controller**\n",
    "\n",
    "   - **Controller**: Criamos uma instância do `Controller`, especificando o uso do roteador **mf**.\n",
    "   - **Roteador MF**: Utiliza um modelo baseado em Fatoração de Matrizes para decidir qual modelo (forte ou fraco) deve responder a cada consulta, com base em dados de preferência e características dos prompts.\n",
    "\n",
    "```python\n",
    "# Define o threshold para decisões de roteamento\n",
    "threshold = 0.5  # Quanto menor o threshold, mais o modelo forte é utilizado\n",
    "```\n",
    "\n",
    "5. **Definição do Threshold**\n",
    "\n",
    "   - **Threshold**: Valor que determina o ponto de corte para usar o modelo forte.\n",
    "     - **Threshold Baixo**: O modelo forte é usado com mais frequência.\n",
    "     - **Threshold Alto**: O modelo fraco é preferido, economizando recursos.\n",
    "   - **Valor Atual**: Com `threshold = 0.5`, o modelo forte é usado quando há pelo menos 50% de chance de oferecer uma resposta significativamente melhor.\n",
    "\n",
    "```python\n",
    "def get_response(prompt):\n",
    "    # Gera a resposta usando o RouteLLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=f\"router-mf-{threshold}\",  # Especifica o roteador e o threshold\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    model_used = response.model  # Obtém o modelo que foi utilizado\n",
    "    content = response.choices[0].message.content\n",
    "    return model_used, content\n",
    "```\n",
    "\n",
    "6. **Função para Obter Respostas**\n",
    "\n",
    "   - **get_response**: Função que envia o prompt ao RouteLLM e retorna a resposta junto com o modelo utilizado.\n",
    "   - **Chamadas ao Modelo**: Usamos o método `create` para gerar uma resposta, especificando o modelo no formato `router-mf-{threshold}`.\n",
    "\n",
    "```python\n",
    "prompts = [\n",
    "    \"Descreva a equação de Navier-Stokes\",\n",
    "    \"Qual o maior país do mundo em extensão territorial?\",\n",
    "    \"Calcule o vórtice de Burgers–Rott\",\n",
    "    \"Qual é a capital do Brasil?\"\n",
    "]\n",
    "```\n",
    "\n",
    "7. **Lista de Prompts**\n",
    "\n",
    "   - **Diversidade de Consultas**: Incluímos prompts de diferentes níveis de complexidade para testar o roteamento.\n",
    "     - **Consultas Complexas**: Podem exigir o modelo forte.\n",
    "     - **Consultas Simples**: Podem ser adequadamente respondidas pelo modelo fraco.\n",
    "\n",
    "```python\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        model_used, response = get_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Modelo usado: {model_used}\")\n",
    "        print(f\"Resposta: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao processar o prompt: {prompt}\")\n",
    "        print(f\"Erro: {e}\\n\")\n",
    "```\n",
    "\n",
    "8. **Processamento dos Prompts**\n",
    "\n",
    "   - **Iteração**: Para cada prompt, obtemos a resposta e identificamos qual modelo foi usado.\n",
    "   - **Exibição**: Imprimimos o prompt, o modelo utilizado e a resposta correspondente.\n",
    "   - **Tratamento de Erros**: Capturamos exceções para garantir que o script continue executando mesmo se ocorrer algum erro.\n",
    "\n",
    "---\n",
    "\n",
    "**Conceitos Fundamentais de LLM Routing com RouteLLM**\n",
    "\n",
    "1. **Roteamento Baseado em Preferência**\n",
    "\n",
    "   - **Objetivo**: Alocar recursos computacionais de forma eficiente, usando modelos mais caros apenas quando necessário.\n",
    "   - **Decisão Automatizada**: O roteador determina qual modelo usar com base na previsão de qual fornecerá a melhor resposta para o prompt dado.\n",
    "\n",
    "2. **Funcionamento do Roteador MF**\n",
    "\n",
    "   - **Fatoração de Matrizes**: Técnica de aprendizado de máquina que decompõe interações complexas em fatores latentes.\n",
    "   - **Dados de Treinamento**: O modelo é treinado em dados onde humanos avaliaram qual modelo oferece melhores respostas para diferentes prompts.\n",
    "   - **Predição da Taxa de Vitória**: Calcula a probabilidade de o modelo forte superar o modelo fraco para o prompt atual.\n",
    "\n",
    "3. **Threshold e Tomada de Decisão**\n",
    "\n",
    "   - **Threshold (Limite)**: Valor que serve como referência para decidir qual modelo usar.\n",
    "   - **Interpretação**:\n",
    "     - **Taxa de Vitória ≥ Threshold**: Usa o modelo forte.\n",
    "     - **Taxa de Vitória < Threshold**: Usa o modelo fraco.\n",
    "   - **Ajuste Estratégico**: O threshold pode ser ajustado para equilibrar custo e desempenho conforme as necessidades do sistema.\n",
    "\n",
    "4. **Benefícios do Roteamento**\n",
    "\n",
    "   - **Eficiência de Custos**: Reduz o uso desnecessário de modelos caros para consultas simples.\n",
    "   - **Manutenção da Qualidade**: Garante que consultas complexas recebam respostas de alta qualidade.\n",
    "   - **Escalabilidade**: Permite lidar com um grande volume de consultas de maneira otimizada.\n",
    "\n",
    "---\n",
    "\n",
    "**Exemplos Práticos do Script**\n",
    "\n",
    "- **Prompt 1**: *\"Descreva a equação de Navier-Stokes\"*\n",
    "  - **Análise**: Tópico avançado em mecânica dos fluidos.\n",
    "  - **Expectativa**: O roteador provavelmente usará o modelo forte (`gpt-4o`) para fornecer uma explicação detalhada.\n",
    "\n",
    "- **Prompt 2**: *\"Qual o maior país do mundo em extensão territorial?\"*\n",
    "  - **Análise**: Pergunta factual simples.\n",
    "  - **Expectativa**: O modelo fraco (`gpt-4o-mini`) deve ser suficiente para responder corretamente.\n",
    "\n",
    "- **Prompt 3**: *\"Calcule o vórtice de Burgers–Rott\"*\n",
    "  - **Análise**: Requer conhecimento especializado e cálculos complexos.\n",
    "  - **Expectativa**: O modelo forte será selecionado para lidar com a complexidade.\n",
    "\n",
    "- **Prompt 4**: *\"Qual é a capital do Brasil?\"*\n",
    "  - **Análise**: Informação geral e direta.\n",
    "  - **Expectativa**: O modelo fraco pode responder adequadamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# Configuração da chave de API da OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file 'config.example.yaml' not found. Using default configuration.\n",
      "Prompt: Descreva a equação de Navier-Stokes\n",
      "Model used: gpt-4o-2024-08-06\n",
      "Response: A equação de Navier-Stokes descreve o movimento de fluidos. É uma equação fundamental em mecânica dos fluidos e tem ampla aplicação em engenharia, meteorologia, oceanografia e muitos outros campos. A equação é baseada na aplicação das leis de conservação de massa, momento e energia a um volume de controle no fluido. A forma mais geral da equação de Navier-Stokes para um fluido incompressível e newtoniano é a seguinte:\n",
      "\n",
      "\\[\n",
      "\\rho \\left( \\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u} \\cdot \\nabla) \\mathbf{u} \\right) = -\\nabla p + \\mu \\nabla^2 \\mathbf{u} + \\mathbf{f}\n",
      "\\]\n",
      "\n",
      "Onde:\n",
      "- \\(\\mathbf{u}\\) é o vetor velocidade do fluido.\n",
      "- \\(t\\) é o tempo.\n",
      "- \\(\\rho\\) é a densidade do fluido.\n",
      "- \\(p\\) é a pressão do fluido.\n",
      "- \\(\\mu\\) é a viscosidade dinâmica do fluido.\n",
      "- \\(\\nabla\\) é o operador nabla (que representa gradiente).\n",
      "- \\(\\nabla^2\\) é o operador Laplaciano.\n",
      "- \\(\\mathbf{f}\\) representa forças externas aplicadas por unidade de volume (como gravidade).\n",
      "\n",
      "Esta equação inclui termos que representam a aceleração do fluido, a advecção (transporte de quantidade de movimento pelo fluxo), o gradiente de pressão, a dissipação viscosa e as forças externas. Devido à sua complexidade, a solução analítica desta equação é conhecida apenas para casos muito simples. Em geral, ela é resolvida numericamente usando métodos computacionais.\n",
      "\n",
      "Prompt: Qual o maior país do mundo em extensão territorial?\n",
      "Model used: gpt-4o-mini-2024-07-18\n",
      "Response: O maior país do mundo em extensão territorial é a Rússia. Com uma área de aproximadamente 17,1 milhões de quilômetros quadrados, a Rússia ocupa uma parte significativa do continente europeu e da Ásia, sendo o país com a maior superfície terrestre do planeta.\n",
      "\n",
      "Prompt: Calcule o vórtice de Burgers–Rott\n",
      "Model used: gpt-4o-2024-08-06\n",
      "Response: O vórtice de Burgers-Rott é uma solução para as equações de Navier-Stokes em um fluido incompressível, representando um vórtice em rotação e alongamento axial. É comumente usado para estudar a estrutura de fluxo em torno de vórtices que sofrem alongamento ou compressão.\n",
      "\n",
      "As equações simplificadas para o vórtice de Burgers-Rott, em coordenadas cilíndricas \\((r, \\theta, z)\\), onde \\(r\\) é a distância radial ao eixo do vórtice, \\(\\theta\\) é o ângulo azimutal, e \\(z\\) é a coordenada axial, são dadas por:\n",
      "\n",
      "1. Componente radial da velocidade: \\( u_r = -\\alpha r \\)\n",
      "2. Componente azimutal da velocidade: \\( u_\\theta = \\frac{\\Gamma}{2\\pi r}(1 - e^{-\\frac{r^2}{4\\nu t}}) \\)\n",
      "3. Componente axial da velocidade: \\( u_z = 2\\alpha z \\)\n",
      "\n",
      "Aqui, \\(\\alpha\\) é uma constante que representa a taxa de alongamento/compressão axial, \\(\\Gamma\\) é a circulação total do vórtice, \\(\\nu\\) é a viscosidade cinemática do fluido, e \\(t\\) é o tempo.\n",
      "\n",
      "A importância do vórtice de Burgers-Rott está na sua capacidade de modelar o comportamento de fluxo em torno de vórtices, considerando tanto a dissipação viscosa quanto o efeito de alongamento/compressão axial, tornando-o relevante para a análise de fenômenos aerodinâmicos e em fluidos geofísicos.\n",
      "\n",
      "Se precisar de mais detalhes sobre a derivação ou aplicações específicas do vórtice de Burgers-Rott, sinta-se à vontade para perguntar!\n",
      "\n",
      "Prompt: Qual é a capital do Brasil?\n",
      "Model used: gpt-4o-mini-2024-07-18\n",
      "Response: A capital do Brasil é Brasília. A cidade foi inaugurada em 21 de abril de 1960 e foi planejada para ser a nova capital do país, com o objetivo de promover o desenvolvimento do interior do Brasil.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from routellm.controller import Controller\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Define the strong and weak models\n",
    "strong_model = \"gpt-4o\"      # Strong model (more capable, higher cost)\n",
    "weak_model = \"gpt-4o-mini\"   # Weak model (less capable, lower cost)\n",
    "\n",
    "# Load the config from the YAML file\n",
    "try:\n",
    "    with open('config.example.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Configuration file 'config.example.yaml' not found. Using default configuration.\")\n",
    "    config = None\n",
    "\n",
    "# Initialize the Controller with the MF router\n",
    "client = Controller(\n",
    "    routers=[\"mf\"],          # Use the Matrix Factorization router\n",
    "    strong_model=strong_model,\n",
    "    weak_model=weak_model,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Set the threshold for routing decisions\n",
    "threshold = 0.3  # The lower the threshold, the more the strong model is used\n",
    "\n",
    "def get_response(prompt):\n",
    "    # Generate response using RouteLLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=f\"router-mf-{threshold}\",  # Specify router and threshold\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    model_used = response.model  # Get the model that was used\n",
    "    content = response.choices[0].message.content\n",
    "    return model_used, content\n",
    "\n",
    "prompts = [\n",
    "    \"Descreva a equação de Navier-Stokes\",\n",
    "    \"Qual o maior país do mundo em extensão territorial?\",\n",
    "    \"Calcule o vórtice de Burgers–Rott\",\n",
    "    \"Qual é a capital do Brasil?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        model_used, response = get_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Model used: {model_used}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the prompt: {prompt}\")\n",
    "        print(f\"Error: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "O script exemplifica como implementar o roteamento de LLMs usando o RouteLLM para otimizar a utilização de modelos de linguagem. Ao empregar o roteador de Fatoração de Matrizes, o sistema pode tomar decisões inteligentes sobre qual modelo utilizar para cada consulta, baseando-se em previsões estatísticas e dados de preferência.\n",
    "\n",
    "**Vantagens Chave:**\n",
    "\n",
    "- **Custo-Efetividade**: Uso eficiente de recursos computacionais, reduzindo despesas operacionais.\n",
    "- **Qualidade das Respostas**: Mantém alto padrão de qualidade onde é mais necessário.\n",
    "- **Flexibilidade**: Possibilidade de ajustar o threshold para atender a diferentes prioridades (custo vs. desempenho).\n",
    "\n",
    "Este conceito é especialmente relevante em aplicações em que o volume de consultas é grande e variado, permitindo escalabilidade sem comprometer a experiência do usuário.\n",
    "\n",
    "\n",
    "**Considerações Finais**\n",
    "\n",
    "O roteamento de LLMs com o RouteLLM representa uma abordagem estratégica para maximizar o valor obtido de modelos de linguagem, alinhando capacidades técnicas com objetivos de negócio. Compreender e aplicar esses conceitos pode levar ao desenvolvimento de sistemas mais inteligentes, eficientes e sustentáveis.\n",
    "\n",
    "Se houver interesse em aprofundar em aspectos específicos ou em como adaptar esse framework a outras necessidades, estou à disposição para auxiliar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoras_academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
