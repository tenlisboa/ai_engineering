{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explicação do Script Abaixo\n",
    "1. **Definição do Modelo**: Criamos uma rede neural simples com duas camadas lineares e uma função de ativação ReLU.\n",
    "2. **Função de Perda**: Utilizamos a perda de erro quadrático médio (MSELoss) para medir a diferença entre as previsões do modelo e os valores reais.\n",
    "3. **Otimizador Adam**: Configuramos o otimizador Adam com uma taxa de aprendizado de 0.001.\n",
    "4. **Dados de Exemplo**: Geramos dados aleatórios para `inputs` e `targets` para simular um conjunto de dados de treinamento.\n",
    "5. **Loop de Treinamento**: \n",
    "   - **Forward Pass**: Calcula as previsões do modelo.\n",
    "   - **Cálculo da Perda**: Calcula a perda entre as previsões e os alvos.\n",
    "   - **Backward Pass**: Calcula os gradientes.\n",
    "   - **Atualização dos Parâmetros**: Atualiza os parâmetros do modelo com base nos gradientes.\n",
    "   - **Impressão da Perda**: Imprime a perda a cada 10 épocas para monitorar o progresso do treinamento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [10/100], Perda: 1.2963\n",
      "Época [20/100], Perda: 1.2542\n",
      "Época [30/100], Perda: 1.2174\n",
      "Época [40/100], Perda: 1.1851\n",
      "Época [50/100], Perda: 1.1552\n",
      "Época [60/100], Perda: 1.1270\n",
      "Época [70/100], Perda: 1.1010\n",
      "Época [80/100], Perda: 1.0776\n",
      "Época [90/100], Perda: 1.0564\n",
      "Época [100/100], Perda: 1.0355\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definindo um modelo simples\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),  # Camada linear com 10 entradas e 5 saídas\n",
    "    nn.ReLU(),         # Função de ativação ReLU\n",
    "    nn.Linear(5, 1)    # Camada linear com 5 entradas e 1 saída\n",
    ")\n",
    "\n",
    "# Definindo a função de perda (Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Definindo o otimizador Adam com uma taxa de aprendizado de 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Gerando dados de exemplo\n",
    "inputs = torch.randn(100, 10)  # 100 amostras, cada uma com 10 características\n",
    "targets = torch.randn(100, 1)  # 100 alvos, cada um com 1 valor\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()  # Zera os gradientes acumulados\n",
    "\n",
    "    outputs = model(inputs)  # Forward pass: calcula as previsões do modelo\n",
    "    loss = criterion(outputs, targets)  # Calcula a perda entre as previsões e os alvos\n",
    "\n",
    "    loss.backward()  # Backward pass: calcula os gradientes\n",
    "    optimizer.step()  # Atualiza os parâmetros do modelo\n",
    "\n",
    "    # Imprime a perda a cada 10 épocas\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Época [{epoch + 1}/100], Perda: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim, em geral, quanto menor o valor da perda, melhor. A perda (ou erro) é uma medida de quão bem o modelo está se ajustando aos dados de treinamento. Valores menores de perda indicam que o modelo está fazendo previsões mais precisas.\n",
    "\n",
    "### Explicação do Resultado\n",
    "Os valores de perda que você obteve mostram uma tendência decrescente ao longo das épocas de treinamento:\n",
    "\n",
    "```\n",
    "Época [10/100], Perda: 1.2963\n",
    "Época [20/100], Perda: 1.2542\n",
    "Época [30/100], Perda: 1.2174\n",
    "Época [40/100], Perda: 1.1851\n",
    "Época [50/100], Perda: 1.1552\n",
    "Época [60/100], Perda: 1.1270\n",
    "Época [70/100], Perda: 1.1010\n",
    "Época [80/100], Perda: 1.0776\n",
    "Época [90/100], Perda: 1.0564\n",
    "Época [100/100], Perda: 1.0355\n",
    "```\n",
    "\n",
    "### O Que Isso Significa\n",
    "1. **Tendência Decrescente**: A perda está diminuindo ao longo das épocas, o que é um bom sinal. Isso indica que o modelo está aprendendo e melhorando suas previsões com o tempo.\n",
    "2. **Convergência**: A perda não está diminuindo drasticamente nas últimas épocas, sugerindo que o modelo está se aproximando de um ponto de convergência. Isso significa que ele está encontrando um bom ajuste para os dados de treinamento.\n",
    "3. **Eficiência do Otimizador**: O otimizador Adam está funcionando bem, ajustando os parâmetros do modelo de forma eficaz para minimizar a perda.\n",
    "\n",
    "### Considerações Finais\n",
    "- **Monitoramento Contínuo**: É importante continuar monitorando a perda para garantir que o modelo não esteja superajustando (overfitting) aos dados de treinamento. Isso pode ser feito avaliando o modelo em um conjunto de dados de validação.\n",
    "- **Ajustes Futuros**: Se a perda parar de diminuir ou começar a aumentar, pode ser necessário ajustar a taxa de aprendizado ou outros hiperparâmetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoras_academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
