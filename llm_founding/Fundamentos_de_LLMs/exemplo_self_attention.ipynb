{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O Nascimento do Mecanismo de Atenção**\n",
    "\n",
    "Um dos motivos pelos quais gosto de assistir palestras de grandes especialistas, mesmo sobre temas que já conheço bem, é ver como eles apresentam suas ideias e ouvir algumas histórias interessantes. Além disso, é sempre bom escutar pessoas inteligentes.\n",
    "\n",
    "Em um vídeo introdutório sobre transformadores, Andrej Karpathy menciona o artigo de 2015, \"Neural Machine Translation by Jointly Learning to Align and Translate,\" que deu início ao mecanismo de atenção. Ele também fala sobre algumas trocas de e-mails com Dzmitry Bahdanau, o autor principal do artigo. Dzmitry enviou a Andrej um e-mail longo explicando o contexto de como surgiu a ideia da atenção (a partir de 18:36 no vídeo — link abaixo, no Youtube).\n",
    "\n",
    "Na época, já se conhecia o gargalo entre o codificador e o decodificador, e muitos experimentos fracassados haviam sido feitos. Dzmitry, refletindo sobre como ele mesmo traduz entre idiomas, percebeu que sempre mudava seu foco entre as línguas de origem e destino. Para gerar a próxima palavra na tradução — ou entender por que uma certa palavra foi usada — era necessário olhar para várias palavras da frase original. Ele modelou essa mudança de foco usando softmax, e, surpreendentemente, funcionou na primeira tentativa. O resto é a história que todos conhecemos!\n",
    "\n",
    "Curiosamente, o termo \"atenção\" foi sugerido por Yoshua Bengio em uma das últimas edições do artigo. Será que a ideia teria ganhado tanta força sem um nome tão atraente? Essa é a sabedoria dos grandes. Quase posso visualizar o momento: \"Certo, Dzmitry, mostre o que você tem. Ah, isso é Atenção!\"\n",
    "\n",
    "Convenhamos, o nome foi genial!\n",
    "\n",
    "Link para a aula: https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1115s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase original: O gato está no tapete\n",
      "\n",
      "Pesos de Atenção:\n",
      "\n",
      "Para a palavra 'O':\n",
      "  Atenção para 'O': 0.3302\n",
      "  Atenção para 'gato': 0.2707\n",
      "  Atenção para 'está': 0.2431\n",
      "  Atenção para 'no': 0.2600\n",
      "  Atenção para 'tapete': 0.1965\n",
      "\n",
      "Para a palavra 'gato':\n",
      "  Atenção para 'O': 0.1781\n",
      "  Atenção para 'gato': 0.1835\n",
      "  Atenção para 'está': 0.1683\n",
      "  Atenção para 'no': 0.1822\n",
      "  Atenção para 'tapete': 0.1666\n",
      "\n",
      "Para a palavra 'está':\n",
      "  Atenção para 'O': 0.2293\n",
      "  Atenção para 'gato': 0.2412\n",
      "  Atenção para 'está': 0.2910\n",
      "  Atenção para 'no': 0.2255\n",
      "  Atenção para 'tapete': 0.2633\n",
      "\n",
      "Para a palavra 'no':\n",
      "  Atenção para 'O': 0.1501\n",
      "  Atenção para 'gato': 0.1598\n",
      "  Atenção para 'está': 0.1380\n",
      "  Atenção para 'no': 0.1856\n",
      "  Atenção para 'tapete': 0.1481\n",
      "\n",
      "Para a palavra 'tapete':\n",
      "  Atenção para 'O': 0.1123\n",
      "  Atenção para 'gato': 0.1448\n",
      "  Atenção para 'está': 0.1596\n",
      "  Atenção para 'no': 0.1467\n",
      "  Atenção para 'tapete': 0.2255\n",
      "\n",
      "Explicação:\n",
      "'O' presta mais atenção em 'O'\n",
      "'gato' presta mais atenção em 'gato'\n",
      "'está' presta mais atenção em 'está'\n",
      "'no' presta mais atenção em 'no'\n",
      "'tapete' presta mais atenção em 'tapete'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Frase de exemplo\n",
    "frase = \"O gato está no tapete\"\n",
    "\n",
    "# Função para criar embeddings simples (apenas para demonstração)\n",
    "def criar_embedding(palavra):\n",
    "    return np.random.rand(4)  # Vetor de 4 dimensões para cada palavra\n",
    "\n",
    "# Função softmax simplificada\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Criando embeddings para cada palavra\n",
    "palavras = frase.split()\n",
    "embeddings = [criar_embedding(palavra) for palavra in palavras]\n",
    "\n",
    "# Função de self-attention simplificada\n",
    "def self_attention(embeddings):\n",
    "    # Criando matrizes Q, K, V (simplificadas)\n",
    "    Q = np.array(embeddings)\n",
    "    K = np.array(embeddings)\n",
    "    V = np.array(embeddings)\n",
    "    \n",
    "    # Calculando os scores de atenção\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Aplicando softmax para obter os pesos de atenção\n",
    "    pesos_atencao = softmax(scores)\n",
    "    \n",
    "    # Calculando os valores ponderados\n",
    "    valores_ponderados = np.dot(pesos_atencao, V)\n",
    "    \n",
    "    return pesos_atencao, valores_ponderados\n",
    "\n",
    "# Aplicando self-attention\n",
    "pesos_atencao, valores_ponderados = self_attention(embeddings)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(\"Frase original:\", frase)\n",
    "print(\"\\nPesos de Atenção:\")\n",
    "for i, palavra in enumerate(palavras):\n",
    "    print(f\"\\nPara a palavra '{palavra}':\")\n",
    "    for j, outra_palavra in enumerate(palavras):\n",
    "        print(f\"  Atenção para '{outra_palavra}': {pesos_atencao[i][j]:.4f}\")\n",
    "\n",
    "print(\"\\nExplicação:\")\n",
    "for i, palavra in enumerate(palavras):\n",
    "    max_atencao = np.argmax(pesos_atencao[i])\n",
    "    print(f\"'{palavra}' presta mais atenção em '{palavras[max_atencao]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo Simplificado de Self-Attention\n",
    "\n",
    "Este notebook demonstra um exemplo simplificado de como o self-attention funciona em uma frase básica.\n",
    "\n",
    "## Visão Geral\n",
    "\n",
    "- Usamos a frase \"O gato está no tapete\" como exemplo.\n",
    "- Criamos embeddings simples para cada palavra.\n",
    "- Implementamos uma versão simplificada do self-attention.\n",
    "- Analisamos como cada palavra se relaciona com as outras na frase.\n",
    "\n",
    "## Componentes Principais\n",
    "\n",
    "1. **Criação de Embeddings**\n",
    "   - Função `criar_embedding`: Gera vetores aleatórios para representar palavras.\n",
    "   - Em modelos reais, estes embeddings seriam aprendidos durante o treinamento.\n",
    "\n",
    "2. **Função Softmax**\n",
    "   - Converte scores em probabilidades.\n",
    "\n",
    "3. **Self-Attention Simplificado**\n",
    "   - Função `self_attention`: Simula o processo de self-attention.\n",
    "   - Cria matrizes Q, K e V simplificadas.\n",
    "   - Calcula scores de atenção e aplica softmax.\n",
    "   - Calcula valores ponderados.\n",
    "\n",
    "## Processo\n",
    "\n",
    "1. Dividimos a frase em palavras.\n",
    "2. Criamos embeddings para cada palavra.\n",
    "3. Aplicamos o self-attention aos embeddings.\n",
    "4. Analisamos os pesos de atenção resultantes.\n",
    "\n",
    "## Resultados\n",
    "\n",
    "O código exibe:\n",
    "- Pesos de atenção entre cada par de palavras.\n",
    "- Uma interpretação simplificada de qual palavra recebe mais atenção de cada palavra da frase.\n",
    "\n",
    "## Observações\n",
    "\n",
    "- Este é um exemplo muito simplificado para fins didáticos.\n",
    "- Em um Transformer real, o processo seria mais complexo:\n",
    "  - Múltiplas camadas de atenção.\n",
    "  - Múltiplas cabeças de atenção.\n",
    "  - Treinamento para aprender embeddings e pesos.\n",
    "\n",
    "Este exemplo ajuda a visualizar como o contexto é considerado no processamento de linguagem natural usando self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase original: Scoras é uma empresa que cria Agentes de Inteligência Artificial\n",
      "\n",
      "Pesos de Atenção:\n",
      "\n",
      "Para a palavra 'Scoras':\n",
      "  Atenção para 'Scoras': 0.1107\n",
      "  Atenção para 'que': 0.1060\n",
      "  Atenção para 'empresa': 0.1020\n",
      "\n",
      "Para a palavra 'é':\n",
      "  Atenção para 'é': 0.1589\n",
      "  Atenção para 'Artificial': 0.1093\n",
      "  Atenção para 'Inteligência': 0.0980\n",
      "\n",
      "Para a palavra 'uma':\n",
      "  Atenção para 'uma': 0.1375\n",
      "  Atenção para 'cria': 0.1139\n",
      "  Atenção para 'empresa': 0.1049\n",
      "\n",
      "Para a palavra 'empresa':\n",
      "  Atenção para 'empresa': 0.0845\n",
      "  Atenção para 'Scoras': 0.0674\n",
      "  Atenção para 'Artificial': 0.0620\n",
      "\n",
      "Para a palavra 'que':\n",
      "  Atenção para 'que': 0.2060\n",
      "  Atenção para 'Scoras': 0.1674\n",
      "  Atenção para 'Agentes': 0.1425\n",
      "\n",
      "Para a palavra 'cria':\n",
      "  Atenção para 'de': 0.1197\n",
      "  Atenção para 'cria': 0.1197\n",
      "  Atenção para 'Inteligência': 0.1179\n",
      "\n",
      "Para a palavra 'Agentes':\n",
      "  Atenção para 'Agentes': 0.2135\n",
      "  Atenção para 'que': 0.1824\n",
      "  Atenção para 'é': 0.1639\n",
      "\n",
      "Para a palavra 'de':\n",
      "  Atenção para 'de': 0.1148\n",
      "  Atenção para 'Inteligência': 0.1117\n",
      "  Atenção para 'cria': 0.1084\n",
      "\n",
      "Para a palavra 'Inteligência':\n",
      "  Atenção para 'Inteligência': 0.1511\n",
      "  Atenção para 'de': 0.1444\n",
      "  Atenção para 'cria': 0.1380\n",
      "\n",
      "Para a palavra 'Artificial':\n",
      "  Atenção para 'é': 0.0978\n",
      "  Atenção para 'Artificial': 0.0950\n",
      "  Atenção para 'empresa': 0.0932\n",
      "\n",
      "Explicação:\n",
      "'Scoras' presta mais atenção em 'Scoras'\n",
      "'é' presta mais atenção em 'é'\n",
      "'uma' presta mais atenção em 'uma'\n",
      "'empresa' presta mais atenção em 'empresa'\n",
      "'que' presta mais atenção em 'que'\n",
      "'cria' presta mais atenção em 'de'\n",
      "'Agentes' presta mais atenção em 'Agentes'\n",
      "'de' presta mais atenção em 'de'\n",
      "'Inteligência' presta mais atenção em 'Inteligência'\n",
      "'Artificial' presta mais atenção em 'é'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Frase de exemplo\n",
    "frase = \"Scoras é uma empresa que cria Agentes de Inteligência Artificial\"\n",
    "\n",
    "# Função para criar embeddings simples (apenas para demonstração)\n",
    "def criar_embedding(palavra):\n",
    "    return np.random.rand(4)  # Vetor de 4 dimensões para cada palavra\n",
    "\n",
    "# Função softmax simplificada\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Criando embeddings para cada palavra\n",
    "palavras = frase.split()\n",
    "embeddings = [criar_embedding(palavra) for palavra in palavras]\n",
    "\n",
    "# Função de self-attention simplificada\n",
    "def self_attention(embeddings):\n",
    "    # Criando matrizes Q, K, V (simplificadas)\n",
    "    Q = np.array(embeddings)\n",
    "    K = np.array(embeddings)\n",
    "    V = np.array(embeddings)\n",
    "    \n",
    "    # Calculando os scores de atenção\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Aplicando softmax para obter os pesos de atenção\n",
    "    pesos_atencao = softmax(scores)\n",
    "    \n",
    "    # Calculando os valores ponderados\n",
    "    valores_ponderados = np.dot(pesos_atencao, V)\n",
    "    \n",
    "    return pesos_atencao, valores_ponderados\n",
    "\n",
    "# Aplicando self-attention\n",
    "pesos_atencao, valores_ponderados = self_attention(embeddings)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(\"Frase original:\", frase)\n",
    "print(\"\\nPesos de Atenção:\")\n",
    "for i, palavra in enumerate(palavras):\n",
    "    print(f\"\\nPara a palavra '{palavra}':\")\n",
    "    # Mostrar apenas as 3 palavras com maior atenção\n",
    "    top_3 = sorted(range(len(palavras)), key=lambda j: pesos_atencao[i][j], reverse=True)[:3]\n",
    "    for j in top_3:\n",
    "        print(f\"  Atenção para '{palavras[j]}': {pesos_atencao[i][j]:.4f}\")\n",
    "\n",
    "print(\"\\nExplicação:\")\n",
    "for i, palavra in enumerate(palavras):\n",
    "    max_atencao = np.argmax(pesos_atencao[i])\n",
    "    print(f\"'{palavra}' presta mais atenção em '{palavras[max_atencao]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo Expandido de Self-Attention\n",
    "\n",
    "Este notebook demonstra um exemplo simplificado de self-attention aplicado a uma frase mais complexa.\n",
    "\n",
    "## Frase de Exemplo\n",
    "\n",
    "\"Scoras é uma empresa que cria Agentes de Inteligência Artificial\"\n",
    "\n",
    "## Comparação com o Exemplo Anterior\n",
    "\n",
    "1. **Tamanho da Frase**\n",
    "   - Nova frase: 9 palavras vs. 5 palavras anteriormente\n",
    "   - Resulta em mais embeddings e uma matriz de atenção maior (9x9 vs. 5x5)\n",
    "\n",
    "2. **Nome Próprio \"Scoras\"**\n",
    "   - Tratado como qualquer outra palavra neste exemplo simplificado\n",
    "   - Em modelos reais, poderia receber tratamento especial\n",
    "\n",
    "3. **Exibição dos Resultados**\n",
    "   - Adaptada para mostrar apenas as 3 palavras com maior atenção para cada palavra\n",
    "   - Melhora a legibilidade com uma frase mais longa\n",
    "\n",
    "4. **Complexidade Computacional**\n",
    "   - Operações matriciais maiores devido ao aumento de palavras\n",
    "\n",
    "5. **Contexto Mais Rico**\n",
    "   - A frase mais longa oferece mais oportunidades para relações entre palavras\n",
    "   - Exemplo: \"Inteligência\" e \"Artificial\" provavelmente terão forte relação\n",
    "\n",
    "6. **Potencial para Relações Mais Complexas**\n",
    "   - Mais palavras permitem capturar relações de longa distância na frase\n",
    "\n",
    "## Observações Importantes\n",
    "\n",
    "- Este exemplo ainda é muito simplificado comparado a modelos reais de NLP:\n",
    "  - Stop words (como \"é\", \"uma\", \"que\") normalmente receberiam tratamento especial\n",
    "  - Nomes próprios como \"Scoras\" poderiam ter embeddings pré-treinados\n",
    "  - A atenção seria calculada em múltiplas camadas e cabeças\n",
    "\n",
    "- Os embeddings aleatórios neste exemplo não produzem relações semanticamente significativas\n",
    "  - Em um modelo treinado, esperaríamos ver relações mais coerentes entre as palavras\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "Este exemplo expandido ilustra como o self-attention pode lidar com frases mais longas e complexas, mesmo que de forma simplificada. Ele demonstra a flexibilidade do mecanismo de atenção em processar entradas de diferentes tamanhos e complexidades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoras_academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
