{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#https://www.cursor.com/\n","#https://docs.anaconda.com/miniconda/\n","#conda create -n myenv python=3.10 (eu estou usando python 3.12.5)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","O script implementa uma versão simplificada do modelo Transformer, que é uma arquitetura de rede neural usada principalmente para tarefas de processamento de linguagem natural. Aqui estão os principais componentes:\n","1. Função softmax: Converte um vetor de números em uma distribuição de probabilidade.\n","Atenção de produto escalar escalado: Calcula a atenção entre consultas (Q), chaves (K) e valores (V).\n","Atenção Multi-Cabeça: Permite que o modelo se concentre em diferentes partes da entrada simultaneamente.\n","Rede Feed-Forward: Processa as saídas da camada de atenção.\n","Camada do Codificador: Combina a atenção multi-cabeça e a rede feed-forward.\n","Transformer: Combina várias camadas de codificador e adiciona uma camada de embedding.\n","O fluxo de dados no modelo é o seguinte:\n","A entrada é convertida em embeddings.\n","Cada camada do codificador processa os dados:\n","a. A atenção multi-cabeça calcula a atenção.\n","b. A rede feed-forward processa o resultado da atenção.\n","3. A saída final representa a codificação da sequência de entrada.\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Forma da entrada: (2, 10)\n","Forma da saída: (2, 10, 64)\n"]}],"source":["import numpy as np\n","\n","# Função softmax para converter valores em probabilidades\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n","\n","# Função de atenção de produto escalar escalado\n","def scaled_dot_product_attention(Q, K, V):\n","    # Calcular o fator de escala\n","    d_k = K.shape[-1]\n","    # Calcular os scores de atenção\n","    attention_scores = np.einsum('...ij,...kj->...ik', Q, K) / np.sqrt(d_k)\n","    # Aplicar softmax para obter as probabilidades de atenção\n","    attention_probs = softmax(attention_scores)\n","    # Calcular a saída ponderada\n","    output = np.einsum('...ij,...jk->...ik', attention_probs, V)\n","    return output\n","\n","# Classe de Atenção Multi-Cabeça\n","class MultiHeadAttention:\n","    def __init__(self, d_model, num_heads):\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        self.depth = d_model // num_heads\n","        \n","        # Inicializar as matrizes de peso\n","        self.WQ = np.random.randn(d_model, d_model)\n","        self.WK = np.random.randn(d_model, d_model)\n","        self.WV = np.random.randn(d_model, d_model)\n","        self.WO = np.random.randn(d_model, d_model)\n","    \n","    # Função para dividir as cabeças de atenção\n","    def split_heads(self, x):\n","        batch_size = x.shape[0]\n","        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n","        return x.transpose(0, 2, 1, 3)\n","    \n","    # Função de propagação para frente\n","    def forward(self, Q, K, V):\n","        batch_size = Q.shape[0]\n","        \n","        # Aplicar as transformações lineares\n","        Q = np.matmul(Q, self.WQ)\n","        K = np.matmul(K, self.WK)\n","        V = np.matmul(V, self.WV)\n","        \n","        # Dividir em múltiplas cabeças\n","        Q = self.split_heads(Q)\n","        K = self.split_heads(K)\n","        V = self.split_heads(V)\n","        \n","        # Calcular a atenção\n","        attention_output = scaled_dot_product_attention(Q, K, V)\n","        # Reorganizar e concatenar as cabeças\n","        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n","        \n","        # Aplicar a transformação linear final\n","        output = np.matmul(attention_output, self.WO)\n","        return output\n","\n","# Classe da rede Feed-Forward\n","class FeedForward:\n","    def __init__(self, d_model, d_ff):\n","        self.W1 = np.random.randn(d_model, d_ff)\n","        self.W2 = np.random.randn(d_ff, d_model)\n","    \n","    def forward(self, x):\n","        # Aplicar duas transformações lineares com ReLU no meio\n","        return np.matmul(np.maximum(np.matmul(x, self.W1), 0), self.W2)\n","\n","# Classe da Camada do Codificador\n","class EncoderLayer:\n","    def __init__(self, d_model, num_heads, d_ff):\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ff = FeedForward(d_model, d_ff)\n","    \n","    def forward(self, x):\n","        # Aplicar atenção multi-cabeça\n","        attention_output = self.mha.forward(x, x, x)\n","        # Aplicar rede feed-forward\n","        ff_output = self.ff.forward(attention_output)\n","        return ff_output\n","\n","# Classe principal do Transformer\n","class Transformer:\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size):\n","        self.embedding = np.random.randn(vocab_size, d_model)\n","        # Criar várias camadas de codificador\n","        self.layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n","    \n","    def forward(self, x):\n","        # Converter índices em embeddings\n","        x = self.embedding[x]\n","        # Passar pelos codificadores\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","# Exemplo de uso\n","vocab_size = 1000\n","d_model = 64\n","num_heads = 4\n","d_ff = 128\n","num_layers = 2\n","\n","# Criar o modelo Transformer\n","transformer = Transformer(num_layers, d_model, num_heads, d_ff, vocab_size)\n","\n","# Criar uma entrada de exemplo (lote de sequências de índices de tokens)\n","input_seq = np.random.randint(0, vocab_size, size=(2, 10))  # 2 sequências, cada uma com 10 tokens\n","\n","# Processar a entrada através do modelo\n","output = transformer.forward(input_seq)\n","print(\"Forma da entrada:\", input_seq.shape)\n","print(\"Forma da saída:\", output.shape)"]},{"cell_type":"markdown","metadata":{},"source":["\n","# Explicação do Transformer Simplificado\n","\n","## Visão Geral\n","Este script implementa uma versão simplificada do modelo Transformer, uma arquitetura de rede neural amplamente utilizada em tarefas de processamento de linguagem natural.\n","\n","## Componentes Principais\n","\n","1. **Função softmax**\n","   - Converte um vetor de números em uma distribuição de probabilidade.\n","\n","2. **Atenção de Produto Escalar Escalado**\n","   - Calcula a atenção entre consultas (Q), chaves (K) e valores (V).\n","\n","3. **Atenção Multi-Cabeça**\n","   - Permite que o modelo se concentre em diferentes partes da entrada simultaneamente.\n","\n","4. **Rede Feed-Forward**\n","   - Processa as saídas da camada de atenção.\n","\n","5. **Camada do Codificador**\n","   - Combina a atenção multi-cabeça e a rede feed-forward.\n","\n","6. **Transformer**\n","   - Combina várias camadas de codificador e adiciona uma camada de embedding.\n","\n","## Fluxo de Dados\n","\n","1. A entrada é convertida em embeddings.\n","2. Cada camada do codificador processa os dados:\n","   a. A atenção multi-cabeça calcula a atenção.\n","   b. A rede feed-forward processa o resultado da atenção.\n","3. A saída final representa a codificação da sequência de entrada.\n","\n","## Funcionamento\n","\n","- O modelo recebe uma sequência de índices de tokens como entrada.\n","- Esses índices são convertidos em vetores de embedding.\n","- As camadas de codificador processam esses embeddings, aplicando atenção e transformações feed-forward.\n","- A saída final é uma representação codificada da sequência de entrada.\n","\n","## Observações\n","\n","- Este é um modelo simplificado para fins didáticos.\n","- Não inclui características avançadas como mascaramento, normalização de camadas ou treinamento.\n","- Serve como uma introdução aos conceitos fundamentais do Transformer."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"scoras_academy","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":2}
