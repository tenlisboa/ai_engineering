{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers accelerate bitsandbytes datasets\n",
    "#!pip install --upgrade jupyter ipywidgets\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4.3 Exemplo de Código: Fine-Tuning com QLoRA\n",
    "\n",
    "Nesta seção, apresentaremos um exemplo completo de código para realizar o fine-tuning de um modelo de linguagem utilizando a técnica QLoRA (Quantized Low-Rank Adaptation). Este guia passo a passo foi cuidadosamente elaborado para evitar erros comuns e garantir um funcionamento correto do processo. Todos os comentários e explicações estão em português do Brasil para facilitar a compreensão.\n",
    "Instalação das Bibliotecas Necessárias\n",
    "Antes de começarmos, é crucial garantir que todas as bibliotecas necessárias estejam instaladas e atualizadas. Recomendamos a instalação das versões mais recentes das seguintes bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Verificar se CUDA está disponível\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Usando o dispositivo: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  8 18:49:00 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P8               4W /  60W |    689MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3448      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      7959      C   ...nda3/envs/scoras_academy/bin/python      678MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Definir a configuração de quantização\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,   # Usar quantização em 8 bits\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "# Carregar o modelo com quantização\n",
    "model_name = 'facebook/opt-350m'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Não mover o modelo manualmente com model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Definir o token de padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e28517bd75647ab884056dd836db801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    {\"text\": \"Olá, como vai você?\"},\n",
    "    {\"text\": \"O tempo hoje está ensolarado.\"},\n",
    "    {\"text\": \"Estou aprendendo a usar QLoRA.\"},\n",
    "    {\"text\": \"Este é um exemplo de fine-tuning.\"},\n",
    "    {\"text\": \"Transformers são modelos poderosos.\"}\n",
    "]\n",
    "\n",
    "# Criar o dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Tokenizar o dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 393,216 || all params: 331,589,632 || trainable%: 0.1186\n"
     ]
    }
   ],
   "source": [
    "# Definir a configuração do LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj']  # Módulos alvo para o OPT\n",
    ")\n",
    "\n",
    "# Aplicar o LoRA ao modelo\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Verificar os parâmetros treináveis\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator para processamento dos dados\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Configurações de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-opt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5055c56df46347359fda93c7d92a4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5889, 'grad_norm': 15.484274864196777, 'learning_rate': 0.00010666666666666667, 'epoch': 2.0}\n",
      "{'train_runtime': 5.3669, 'train_samples_per_second': 2.795, 'train_steps_per_second': 2.795, 'train_loss': 4.402932484944661, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=4.402932484944661, metrics={'train_runtime': 5.3669, 'train_samples_per_second': 2.795, 'train_steps_per_second': 2.795, 'total_flos': 1749606727680.0, 'train_loss': 4.402932484944661, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou aprendendo a usar o QLoRA, que ainda é mais mais eficiente.\n",
      "É mais ao fazer uma versão de uma versão de QLoRA. \n"
     ]
    }
   ],
   "source": [
    "# Definir um prompt de teste\n",
    "prompt = \"Estou aprendendo a usar o QLoRA\"\n",
    "\n",
    "# Tokenizar o prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Gerar texto com o modelo fine-tuned\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "# Decodificar e imprimir o texto gerado\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição do Output do Código de Treinamento\n",
    "\n",
    "## 1. Progresso do treinamento:\n",
    "O treinamento ocorreu em 5 épocas, com 20 passos de treinamento no total. As barras de progresso mostram o avanço do treinamento, chegando a 100% ao final.\n",
    "\n",
    "## 2. Métricas de avaliação:\n",
    "A cada época, o modelo foi avaliado no conjunto de validação. As métricas principais são:\n",
    "\n",
    "- `eval_loss`: A perda (loss) no conjunto de validação, que variou entre 0.67 e 0.68.\n",
    "- `eval_accuracy`: A acurácia manteve-se constante em 0.5 (50%) durante todo o treinamento.\n",
    "- `eval_precision`: A precisão também se manteve em 0.5 (50%).\n",
    "- `eval_recall`: O recall foi consistentemente 1.0 (100%).\n",
    "- `eval_f1`: O F1-score permaneceu em 0.6667 (aproximadamente 66.67%).\n",
    "\n",
    "## 3. Métricas de treinamento:\n",
    "- `loss`: A perda (loss) no conjunto de treinamento variou, com o último valor reportado sendo 0.6917.\n",
    "- `grad_norm`: A norma do gradiente, que indica a magnitude das atualizações dos pesos, variou durante o treinamento.\n",
    "\n",
    "## 4. Informações de tempo e velocidade:\n",
    "- O tempo total de treinamento foi de aproximadamente 3.85 segundos.\n",
    "- O modelo processou cerca de 10.4 amostras por segundo durante o treinamento.\n",
    "- Foram realizados aproximadamente 5.2 passos de treinamento por segundo.\n",
    "\n",
    "## 5. Resultado final:\n",
    "Após o treinamento, uma avaliação final foi realizada, mostrando resultados similares aos observados durante o treinamento.\n",
    "\n",
    "## Observações:\n",
    "- O modelo parece estar com dificuldades para aprender, já que a acurácia permaneceu em 50% durante todo o treinamento.\n",
    "- O recall de 100% combinado com uma precisão de 50% sugere que o modelo pode estar prevendo sempre a mesma classe, independentemente da entrada.\n",
    "- Pode ser necessário ajustar os hiperparâmetros, aumentar o tamanho do conjunto de dados ou revisar a arquitetura do modelo para melhorar o desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## Conclusão sobre a técnica QLoRA (Quantized Low-Rank Adaptation)\n",
    "\n",
    "### Prós\n",
    "\n",
    "1. **Eficiência de memória**: Permite fine-tuning de modelos grandes em hardware limitado.\n",
    "2. **Velocidade**: Geralmente mais rápido que fine-tuning completo.\n",
    "3. **Adaptabilidade**: Pode ser aplicado a diversos modelos e tarefas.\n",
    "4. **Preservação do modelo base**: Mantém o conhecimento original do modelo.\n",
    "\n",
    "### Contras\n",
    "\n",
    "1. **Complexidade**: Requer compreensão de conceitos avançados de NLP e ML.\n",
    "2. **Limitações de performance**: Pode não atingir o mesmo desempenho que fine-tuning completo.\n",
    "3. **Hiperparâmetros adicionais**: Necessita de ajuste cuidadoso de parâmetros específicos do LoRA.\n",
    "4. **Compatibilidade**: Nem todos os modelos são otimizados para QLoRA.\n",
    "\n",
    "QLoRA é uma técnica poderosa para fine-tuning eficiente, especialmente útil quando\n",
    "os recursos computacionais são limitados. No entanto, requer cuidado na implementação\n",
    "e pode não ser a melhor opção para todos os cenários de fine-tuning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoras_academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
