{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hugging Face Transformers\n",
    "\n",
    "## Introdução ao Hugging Face Transformers\n",
    "\n",
    "O Hugging Face Transformers é uma biblioteca open-source que oferece uma ampla gama de modelos pré-treinados para tarefas de processamento de linguagem natural (NLP). Ela suporta tanto PyTorch quanto TensorFlow, permitindo flexibilidade na escolha do framework.\n",
    "\n",
    "## Utilização para SLMs\n",
    "\n",
    "Para SLMs (Small Language Models), o Hugging Face fornece modelos leves e eficientes, como o DistilBERT e o TinyBERT, que são versões compactas de modelos maiores. Esses modelos são ideais para aplicações com recursos limitados ou que exigem alta velocidade.\n",
    "\n",
    "## Modelos Pré-treinados Disponíveis\n",
    "\n",
    "Alguns modelos pré-treinados disponíveis para SLMs incluem:\n",
    "\n",
    "- **DistilBERT**: Uma versão compacta do BERT com 40% menos parâmetros.\n",
    "- **TinyBERT**: Um modelo ainda menor com apenas 14.5 milhões de parâmetros.\n",
    "- **ALBERT**: Um modelo que reduz a memória necessária compartilhando parâmetros.\n",
    "\n",
    "## Exemplo Prático com Python\n",
    "\n",
    "### Instalação das Bibliotecas Necessárias\n",
    "\n",
    "Antes de começar, é necessário instalar as bibliotecas apropriadas para trabalhar com o Hugging Face Transformers.\n",
    "\n",
    "Este markdown mantém a estrutura organizada e clara, removendo a linha específica de instalação conforme solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: torch in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (73.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/anderson/miniconda3/envs/scoras_academy/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch  # ou tensorflow, dependendo do framework escolhido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#carregando o modelo pré-treinado\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Escolha do modelo pré-treinado\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Carregamento do tokenizer e do modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0302,  0.0864]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Texto de entrada\n",
    "text = \"Este é um exemplo de texto para classificação.\"\n",
    "\n",
    "# Tokenização do texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Obtenção das previsões\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Visualização das previsões\n",
    "logits = outputs.logits\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Explicação\n",
    "\n",
    "Este resultado é um tensor PyTorch que representa as saídas brutas (logits) do modelo para a tarefa de classificação. Vamos analisar em detalhes:\n",
    "\n",
    "1. **Formato do tensor**: \n",
    "   - Forma [1, 2]: 1 linha (uma entrada de texto) e 2 colunas (classificação binária).\n",
    "\n",
    "2. **Valores do tensor**:\n",
    "   - Logits brutos: [-0.1819, -0.0352]\n",
    "\n",
    "3. **Significado dos números**:\n",
    "   - Cada número é a \"pontuação\" que o modelo atribui a cada classe.\n",
    "   - O valor mais alto geralmente corresponde à classe prevista.\n",
    "   - Aqui, -0.0352 > -0.1819, indicando leve inclinação para a segunda classe.\n",
    "\n",
    "4. **Interpretação**:\n",
    "   - Para obter probabilidades, aplica-se uma função softmax aos logits.\n",
    "   - A classe com o logit mais alto é considerada a previsão do modelo.\n",
    "\n",
    "5. **grad_fn=<AddmmBackward0>**:\n",
    "   - Parte do sistema de autograd do PyTorch.\n",
    "   - Mantém informações para cálculo de gradientes durante o treinamento.\n",
    "\n",
    "## Observações Importantes\n",
    "\n",
    "- O modelo `distilbert-base-uncased` não foi fine-tuned para uma tarefa específica de classificação.\n",
    "- Isso explica os logits próximos de zero e a falta de preferência forte por qualquer classe.\n",
    "\n",
    "## Próximos Passos para Uso Efetivo\n",
    "\n",
    "1. Fine-tunar o modelo para sua tarefa específica.\n",
    "2. Aplicar softmax aos logits para obter probabilidades.\n",
    "3. Mapear as saídas para rótulos de classe específicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4709, 0.5291]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  E agora, porque os resultados somados são iguais a 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Explicação\n",
    "\n",
    "1. **Transformação dos logits em probabilidades:**\n",
    "   - A função softmax converte os logits brutos em probabilidades, somando 1 e ficando entre 0 e 1.\n",
    "\n",
    "2. **Interpretação dos valores:**\n",
    "   - 0.4634 (46.34%): probabilidade da primeira classe\n",
    "   - 0.5366 (53.66%): probabilidade da segunda classe\n",
    "\n",
    "3. **Relação com os logits originais:**\n",
    "   - Logits originais: [-0.1819, -0.0352]\n",
    "   - O softmax manteve a ordem relativa: o segundo logit era maior, resultando em maior probabilidade\n",
    "\n",
    "4. **Proximidade das probabilidades:**\n",
    "   - Probabilidades próximas (46.34% vs 53.66%)\n",
    "   - Reflete a pequena diferença entre os logits originais\n",
    "\n",
    "5. **Interpretação do modelo:**\n",
    "   - Leve inclinação para a segunda classe (53.66%)\n",
    "   - Confiança não muito alta, dada a proximidade das probabilidades\n",
    "\n",
    "6. **grad_fn=<SoftmaxBackward0>:**\n",
    "   - O PyTorch mantém informações para cálculo de gradientes, útil para treinamento\n",
    "\n",
    "Esta transformação facilita a interpretação, permitindo análise em termos de probabilidades ou porcentagens para cada classe, em vez de valores arbitrários de logits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoras_academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
