{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 2: Técnicas Avançadas de Recuperação de Informação\n",
    "Script 2.1: Implementação de Recuperação Neural com Representações Densas\n",
    "Este script demonstra como utilizar embeddings densos para melhorar a recuperação de documentos por meio de recuperação neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das bibliotecas necessárias (execute apenas se ainda não estiverem instaladas)\n",
    "#!pip install -q sentence-transformers transformers\n",
    "#!pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "#!pip install jupyterlab\n",
    "#!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "#!jupyter nbextension enable --py widgetsnbextension\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento de um conjunto de documentos de exemplo\n",
    "documents = [\n",
    "    \"O aprendizado de máquina é uma subárea da inteligência artificial focada em algoritmos.\",\n",
    "    \"Redes neurais são modelos computacionais inspirados no cérebro humano.\",\n",
    "    \"A recuperação de informação trata de obter informações relevantes a partir de grandes conjuntos de dados.\",\n",
    "    \"Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\",\n",
    "    \"A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar a memória CUDA primeiro\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Forçar o uso da CPU\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "# Cálculo dos embeddings dos documentos usando CPU\n",
    "doc_embeddings = model.encode(documents, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos Recuperados:\n",
      "(Score: 0.8946) - Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\n",
      "(Score: 0.6423) - A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas.\n",
      "(Score: 0.5441) - Redes neurais são modelos computacionais inspirados no cérebro humano.\n"
     ]
    }
   ],
   "source": [
    "# Definição da consulta\n",
    "consulta = \"Como os modelos de linguagem geram texto semelhante ao humano?\"\n",
    "\n",
    "# Cálculo do embedding da consulta\n",
    "consulta_embedding = model.encode(consulta, convert_to_tensor=True)\n",
    "\n",
    "# Cálculo das similaridades\n",
    "cosine_scores = util.cos_sim(consulta_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Obtenção dos índices dos documentos mais similares\n",
    "top_k = 3\n",
    "top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "# Exibição dos documentos recuperados\n",
    "print(\"Documentos Recuperados:\")\n",
    "for score, idx in zip(top_results.values, top_results.indices):\n",
    "    print(f\"(Score: {score:.4f}) - {documents[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta Gerada:\n",
      "Contexto: Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano. A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas. Redes neurais são modelos computacionais inspirados no cérebro humano.\n",
      "Pergunta: Como os modelos de linguagem geram texto semelhante ao humano?\n",
      "Resposta: Me gese ai por oque en el mundo oque la nacional.\n",
      "Cui de gesquierdo: Como os modelos de linguagem açao, es ação. Añalo por sono, da ejeção tenga ser férín.\n",
      "Udo nesque de sua, para ação en udo ném que udo nesque desejadas.\n",
      "Egoção: Oquia, y todos estamos éjos no ajestos egoção.\n",
      "Udo céres que ação ação de mez peremção:\n"
     ]
    }
   ],
   "source": [
    "# Uso de um modelo de linguagem para gerar a resposta\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# Concatenando os documentos recuperados\n",
    "contexto = \" \".join([documents[idx] for idx in top_results.indices])\n",
    "\n",
    "# Construção do prompt para o modelo de linguagem\n",
    "prompt = f\"Contexto: {contexto}\\nPergunta: {consulta}\\nResposta:\"\n",
    "\n",
    "# Geração da resposta\n",
    "resposta = generator(prompt, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Exibição da resposta\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(resposta[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise:\n",
    "\n",
    "Melhorias Observadas:\n",
    "A recuperação neural captura melhor o contexto semântico, recuperando documentos mais relevantes.\n",
    "A resposta gerada tende a ser mais precisa devido à melhor qualidade dos documentos recuperados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script 2.2: Implementação de Pesquisa Híbrida (Combinação de Métodos Esparsos e Densos)\n",
    "Este script demonstra como combinar métodos esparsos (BM25) e densos (embeddings) para melhorar a recuperação de documentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anderson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importações adicionais necessárias\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessário para o NLTK (execute apenas uma vez)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenização dos documentos\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Criação do objeto BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Tokenização da consulta\n",
    "tokenized_query = word_tokenize(consulta.lower())\n",
    "\n",
    "# Agora podemos calcular os scores BM25\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Normalização dos scores BM25\n",
    "scaler = MinMaxScaler()\n",
    "bm25_scores_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo das similaridades densas (Embeddings)\n",
    "cosine_scores = util.cos_sim(consulta_embedding, doc_embeddings)[0].cpu().numpy()\n",
    "\n",
    "# Normalização das similaridades densas\n",
    "cosine_scores_norm = scaler.fit_transform(cosine_scores.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos Recuperados (Pesquisa Híbrida):\n",
      "(Score: 1.0000) - Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\n",
      "(Score: 0.3396) - A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas.\n",
      "(Score: 0.2319) - Redes neurais são modelos computacionais inspirados no cérebro humano.\n"
     ]
    }
   ],
   "source": [
    "# Combinação das similaridades\n",
    "alpha = 0.5  # Peso para a similaridade esparsa\n",
    "beta = 0.5   # Peso para a similaridade densa\n",
    "\n",
    "combined_scores = alpha * bm25_scores_norm + beta * cosine_scores_norm\n",
    "\n",
    "# Obtenção dos índices dos documentos mais similares\n",
    "top_k = 3\n",
    "combined_top_indices = combined_scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "# Exibição dos documentos recuperados\n",
    "print(\"Documentos Recuperados (Pesquisa Híbrida):\")\n",
    "for idx in combined_top_indices:\n",
    "    print(f\"(Score: {combined_scores[idx]:.4f}) - {documents[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson/miniconda3/envs/RAG/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta Gerada:\n",
      "Contexto: Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano. A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas. Redes neurais são modelos computacionais inspirados no cérebro humano.\n",
      "Pergunta: Como os modelos de linguagem geram texto semelhante ao humano?\n",
      "Resposta: São en una por ôn o ôn.\n"
     ]
    }
   ],
   "source": [
    "# Uso do modelo de linguagem para gerar a resposta\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# Concatenando os documentos recuperados\n",
    "contexto = \" \".join([documents[idx] for idx in combined_top_indices])\n",
    "\n",
    "# Construção do prompt para o modelo de linguagem\n",
    "prompt = f\"Contexto: {contexto}\\nPergunta: {consulta}\\nResposta:\"\n",
    "\n",
    "# Geração da resposta\n",
    "resposta = generator(prompt, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "# Exibição da resposta\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(resposta[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Detalhada da Implementação de Busca Híbrida (BM25 + Embeddings Densos)\n",
    "\n",
    "## 1. Visão Geral\n",
    "O código implementa uma estratégia de busca híbrida que combina dois métodos de recuperação de informação:\n",
    "- **Método Esparso**: BM25 (Best Matching 25)\n",
    "- **Método Denso**: Embeddings usando Sentence Transformers\n",
    "\n",
    "## 2. Implementação do BM25 (Método Esparso)\n",
    "```python\n",
    "# Preparação dos documentos\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Processamento da consulta\n",
    "tokenized_query = word_tokenize(consulta.lower())\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Normalização dos scores\n",
    "bm25_scores_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
    "```\n",
    "- O BM25 trabalha com tokens (palavras individuais)\n",
    "- Cada documento é tokenizado e convertido para minúsculas\n",
    "- A consulta passa pelo mesmo processo de tokenização\n",
    "- Os scores são normalizados para escala [0,1] usando MinMaxScaler\n",
    "\n",
    "## 3. Embeddings Densos (Método Denso)\n",
    "```python\n",
    "# Cálculo das similaridades densas\n",
    "cosine_scores = util.cos_sim(consulta_embedding, doc_embeddings)[0].cpu().numpy()\n",
    "cosine_scores_norm = scaler.fit_transform(cosine_scores.reshape(-1, 1)).flatten()\n",
    "```\n",
    "- Utiliza o modelo `all-MiniLM-L6-v2` para gerar embeddings\n",
    "- Calcula similaridade por cosseno entre consulta e documentos\n",
    "- Também normaliza os scores para escala [0,1]\n",
    "\n",
    "## 4. Combinação dos Métodos\n",
    "```python\n",
    "alpha = 0.5  # Peso para similaridade esparsa (BM25)\n",
    "beta = 0.5   # Peso para similaridade densa (Embeddings)\n",
    "combined_scores = alpha * bm25_scores_norm + beta * cosine_scores_norm\n",
    "```\n",
    "- Usa média ponderada dos scores normalizados\n",
    "- Pesos iguais (0.5) para ambos os métodos\n",
    "- Permite ajuste dos pesos conforme necessidade\n",
    "\n",
    "## 5. Resultados e Geração de Resposta\n",
    "Os resultados mostram que a busca híbrida recuperou documentos relevantes:\n",
    "1. \"Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\" (Score: 1.0000)\n",
    "2. \"A engenharia de prompt é crucial para orientar modelos de linguagem...\" (Score: 0.3396)\n",
    "3. \"Redes neurais são modelos computacionais...\" (Score: 0.2319)\n",
    "\n",
    "## 6. Observações sobre a Geração de Resposta\n",
    "- A resposta gerada pelo modelo distilgpt2 não foi satisfatória\n",
    "- O modelo está gerando texto sem sentido, possivelmente porque:\n",
    "  1. Está sendo usado sem fine-tuning para português\n",
    "  2. O prompt pode precisar de melhor engenharia\n",
    "  3. O modelo distilgpt2 pode não ser a melhor escolha para português\n",
    "\n",
    "## 7. Vantagens da Abordagem Híbrida\n",
    "1. **BM25**: Excelente em correspondência exata de palavras-chave\n",
    "2. **Embeddings**: Captura relações semânticas e contextuais\n",
    "3. **Combinação**: Aproveita o melhor dos dois mundos, melhorando a qualidade da recuperação\n",
    "\n",
    "## 8. Possíveis Melhorias\n",
    "1. Ajustar os pesos alpha e beta\n",
    "2. Usar um modelo de linguagem em português\n",
    "3. Melhorar a engenharia de prompt\n",
    "4. Implementar validação cruzada para otimizar os pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
