{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução e Limitações dos Sistemas RAG Convencionais \n",
    "\n",
    "1.1. Contextualização da RAG \n",
    "\n",
    "A Recuperação Aumentada por Geração (RAG) é uma abordagem que combina modelos de linguagem avançados com sistemas de recuperação de informação para produzir respostas mais precisas e relevantes. Enquanto os modelos de linguagem, como o GPT-4o, Gemini,Llama, Qwen e vários outros, possuem um vasto conhecimento adquirido durante o treinamento, eles podem carecer de informações atualizadas ou específicas de um domínio. A RAG aborda essa limitação ao integrar mecanismos de recuperação que buscam dados externos para complementar a geração de texto. Todos esses assuntos forem vistos na Trilha Fundamentos de RAG!  \n",
    "\n",
    "Funcionamento Básico da RAG: \n",
    "\n",
    "Recuperação: Dada uma consulta ou pergunta, o sistema recupera documentos ou informações relevantes de uma base de dados externa. \n",
    "\n",
    "Geração: Utilizando um modelo de linguagem, a resposta é gerada com base tanto na consulta quanto nos documentos recuperados, garantindo que a resposta seja informada e contextualizada. \n",
    "\n",
    "1.2. Desafios dos Sistemas RAG Básicos \n",
    "\n",
    "Apesar de seus benefícios, os sistemas RAG convencionais enfrentam diversos desafios: \n",
    "\n",
    "1.2.1. Alucinações \n",
    "\n",
    "Os modelos de linguagem podem gerar informações não suportadas pelos documentos recuperados, fenômeno conhecido como alucinação. Isso ocorre quando o modelo incorpora dados incorretos ou irrelevantes em suas respostas, comprometendo a confiabilidade do sistema. \n",
    "\n",
    "1.2.2. Relevância Limitada na Recuperação \n",
    "\n",
    "Métodos de recuperação tradicionais, como BM25 ou TF-IDF, baseiam-se na correspondência de palavras-chave e podem não capturar o significado semântico profundo das consultas. Isso pode levar à recuperação de documentos irrelevantes ou menos úteis. \n",
    "\n",
    "1.2.3. Incapacidade de Lidar com Consultas Ambíguas \n",
    "\n",
    "Consultas ambíguas ou incompletas podem resultar em respostas inadequadas. Sistemas RAG básicos podem não possuir mecanismos eficientes para desambiguar ou refinar consultas, afetando a qualidade da informação fornecida. \n",
    "\n",
    "1.3. Impacto das Limitações na Aplicação Prática \n",
    "\n",
    "As limitações mencionadas têm implicações significativas em aplicações do mundo real: \n",
    "\n",
    "Confiabilidade em Domínios Críticos: Em áreas como saúde ou finanças, informações incorretas podem ter consequências graves. Alucinações ou falta de precisão podem comprometer a confiança no sistema. \n",
    "\n",
    "Experiência do Usuário: Respostas imprecisas ou irrelevantes podem frustrar os usuários, diminuindo a eficácia de assistentes virtuais ou chatbots. \n",
    "\n",
    "Adaptação a Domínios Específicos: A incapacidade de lidar com terminologias ou contextos especializados limita a adoção de sistemas RAG em setores especializados.\n",
    "\n",
    "1.4 Exemplos Práticos \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script 1.4.1: Implementação de um Sistema RAG Básico com Recuperação Esparsa (BM25)\n",
    "Este script demonstra a implementação de um sistema RAG básico que utiliza o algoritmo BM25 para recuperação de documentos e um modelo de linguagem simples para geração de respostas. O objetivo é ilustrar como sistemas RAG convencionais operam e identificar possíveis limitações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson/miniconda3/envs/RAG/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/anderson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Documentos\n",
      "0  O aprendizado de máquina é uma subárea da inte...\n",
      "1  Redes neurais são modelos computacionais inspi...\n",
      "2  A recuperação de informação trata de obter inf...\n",
      "3  Modelos de linguagem pré-treinados podem gerar...\n",
      "4  A engenharia de prompt é crucial para orientar...\n",
      "\n",
      "Documentos Recuperados:\n",
      "- Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\n",
      "- A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas.\n",
      "- A recuperação de informação trata de obter informações relevantes a partir de grandes conjuntos de dados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson/miniconda3/envs/RAG/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta Gerada:\n",
      "Contexto: Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano. A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas. A recuperação de informação trata de obter informações relevantes a partir de grandes conjuntos de dados.\n",
      "Pergunta: Como os modelos de linguagem podem gerar texto?\n",
      "Resposta: Como os modelos de linguagem podem gerar texto?\n"
     ]
    }
   ],
   "source": [
    "# Instalação das bibliotecas necessárias (execute apenas se ainda não estiverem instaladas)\n",
    "!pip install -q rank_bm25 transformers pandas numpy nltk\n",
    "\n",
    "# Importação das bibliotecas necessárias\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download dos recursos necessários do NLTK (execute apenas uma vez)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Carregamento de um conjunto de documentos de exemplo\n",
    "documents = [\n",
    "    \"O aprendizado de máquina é uma subárea da inteligência artificial focada em algoritmos.\",\n",
    "    \"Redes neurais são modelos computacionais inspirados no cérebro humano.\",\n",
    "    \"A recuperação de informação trata de obter informações relevantes a partir de grandes conjuntos de dados.\",\n",
    "    \"Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\",\n",
    "    \"A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas.\"\n",
    "]\n",
    "\n",
    "# Exibição dos documentos\n",
    "df = pd.DataFrame(documents, columns=['Documentos'])\n",
    "print(df)\n",
    "\n",
    "# Definição da consulta do usuário\n",
    "consulta = \"Como os modelos de linguagem podem gerar texto?\"\n",
    "\n",
    "# Tokenização dos documentos\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Criação do objeto BM25\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "# Tokenização da consulta\n",
    "tokenized_query = word_tokenize(consulta.lower())\n",
    "\n",
    "# Obtenção das pontuações BM25 para a consulta\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Obtenção dos índices dos documentos mais relevantes\n",
    "top_k = 3  # Número de documentos a serem recuperados\n",
    "top_n = bm25.get_top_n(tokenized_query, documents, n=top_k)\n",
    "\n",
    "# Exibição dos documentos recuperados\n",
    "print(\"\\nDocumentos Recuperados:\")\n",
    "for doc in top_n:\n",
    "    print(f\"- {doc}\")\n",
    "\n",
    "# Uso de um modelo de linguagem para gerar a resposta\n",
    "generator = pipeline('text-generation', \n",
    "                    model='distilgpt2',\n",
    "                    device='cpu',  # Especifica explicitamente o uso da CPU\n",
    "                    truncation=True)  # Ativa explicitamente a truncagem\n",
    "\n",
    "# Concatenando os documentos recuperados\n",
    "contexto = \" \".join(top_n)\n",
    "\n",
    "# Construção do prompt para o modelo de linguagem\n",
    "prompt = f\"Contexto: {contexto}\\nPergunta: {consulta}\\nResposta:\"\n",
    "\n",
    "# Geração da resposta\n",
    "resposta = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=50,  # Usa max_new_tokens em vez de max_length\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,  # Define explicitamente o pad_token_id\n",
    "    clean_up_tokenization_spaces=True  # Define explicitamente\n",
    ")\n",
    "\n",
    "# Exibição da resposta\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(resposta[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pontuações BM25 para cada documento:\n",
      "Documento 0: 0.2198\n",
      "Documento 1: 0.2528\n",
      "Documento 2: 0.3853\n",
      "Documento 3: 4.4616\n",
      "Documento 4: 0.8192\n"
     ]
    }
   ],
   "source": [
    "# Adicione este código após o cálculo dos doc_scores\n",
    "print(\"\\nPontuações BM25 para cada documento:\")\n",
    "for idx, score in enumerate(doc_scores):\n",
    "    print(f\"Documento {idx}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise das Pontuações BM25\n",
    "\n",
    "O resultado mostra que o BM25 funcionou corretamente, onde:\n",
    "O Documento 3 (\"Modelos de linguagem pré-treinados podem gerar texto semelhante ao humano.\") obteve a maior pontuação (4.4616), o que é esperado já que está mais diretamente relacionado à consulta \"Como os modelos de linguagem podem gerar texto?\"\n",
    "O Documento 4 ficou em segundo lugar (0.8192) por mencionar \"modelos de linguagem\"\n",
    "Os outros documentos receberam pontuações baixas (<0.4) por terem menor relevância para a consulta\n",
    "Conclusão: O BM25 foi eficaz em identificar o documento mais relevante para a consulta, demonstrando que a recuperação está funcionando adequadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script 1.4.2: Demonstrando Alucinações em Sistemas RAG Convencionais\n",
    " Este script ilustra como modelos de linguagem podem gerar alucinações quando não estão adequadamente fundamentados nos documentos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos Recuperados:\n",
      "- O aprendizado de máquina é uma subárea da inteligência artificial focada em algoritmos.\n",
      "- A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas.\n",
      "- A recuperação de informação trata de obter informações relevantes a partir de grandes conjuntos de dados.\n"
     ]
    }
   ],
   "source": [
    "# Reutilização dos documentos e do modelo BM25 previamente definidos\n",
    "\n",
    "# Definição de uma consulta que não está coberta pelos documentos\n",
    "consulta = \"Qual é a capital da Irlanda?\"\n",
    "\n",
    "# Tokenização da consulta\n",
    "tokenized_query = word_tokenize(consulta.lower())\n",
    "\n",
    "# Obtenção das pontuações BM25 para a consulta\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Verificação se algum documento relevante foi recuperado\n",
    "if max(doc_scores) == 0:\n",
    "    print(\"Nenhum documento relevante foi recuperado.\")\n",
    "    top_n = []\n",
    "else:\n",
    "    # Obtenção dos documentos mais relevantes\n",
    "    top_n = bm25.get_top_n(tokenized_query, documents, n=top_k)\n",
    "    # Exibição dos documentos recuperados\n",
    "    print(\"Documentos Recuperados:\")\n",
    "    for doc in top_n:\n",
    "        print(f\"- {doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta Gerada:\n",
      "Contexto: O aprendizado de máquina é uma subárea da inteligência artificial focada em algoritmos. A engenharia de prompt é crucial para orientar modelos de linguagem a produzirem respostas desejadas. A recuperação de informação trata de obter informações relevantes a partir de grandes conjuntos de dados.\n",
      "Pergunta: Qual é a capital da Irlanda?\n",
      "Resposta: It is a real life form that is not limited to the social and political elements. It also helps to facilitate exchange between the people, rather than the bureaucracy. I personally prefer to keep it natural and human, but the main goal is to make a\n"
     ]
    }
   ],
   "source": [
    "# Uso do modelo de linguagem para gerar a resposta, mesmo sem contexto\n",
    "\n",
    "# Se não houver documentos recuperados, o contexto será vazio\n",
    "contexto = \" \".join(top_n)\n",
    "\n",
    "# Construção do prompt para o modelo de linguagem\n",
    "prompt = f\"Contexto: {contexto}\\nPergunta: {consulta}\\nResposta:\"\n",
    "\n",
    "# Geração da resposta\n",
    "resposta = generator(prompt, max_new_tokens=50, num_return_sequences=1, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "\n",
    "# Exibição da resposta\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(resposta[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise:\n",
    "\n",
    "Observação da Alucinação:\n",
    "O modelo tenta responder à pergunta sem informações relevantes, possivelmente fornecendo uma resposta incorreta ou inventada.\n",
    "Isso demonstra a limitação dos sistemas RAG convencionais em garantir respostas precisas quando a recuperação falha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
